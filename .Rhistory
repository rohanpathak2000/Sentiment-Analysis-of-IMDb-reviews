train <- Boston[train_samples,]
test <- Boston[-train_samples,]
#Building a regression model
model <- lm(medv~.,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~.,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~.,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9+class,data=train)
summary(model)
table(df$class)
data(biopsy)
df <- biopsy
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
table(df$class)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9+class,data=train)
summary(model)
table(df$class)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1 ~ .,data=train)
summary(model)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9+class,data=train)
summary(model)
pacman::p_load(MASS)
data(biopsy)
df <- biopsy
str(df)
pacman::p_load(tidyverse)
pacman::p_load(caret)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9+class,data=train)
summary(model)
print("RSE")
print(sigma(model))
#Make predictions
pred <- model %>%
predict(test)
#model performance
RMSE <- RMSE(pred,test$V1)
RMSE
#model performance
RMSE <- RMSE(pred,test$V1)
RMSE
#Make predictions
pred <- model %>%
predict(test$V2)
#Make predictions
pred <- model %>%
predict(test)
pred
#model performance
RMSE <- RMSE(pred,test$V1)
RMSE
#model performance
RMSE <- RMSE(test$V1,pred)
RMSE
pacman::p_load(MASS)
data(biopsy)
df <- biopsy
str(df)
df$class <- ifelse(df$class == "benign", 0, 1)
set.seed(123)
train_samples <- df$V1 %>%
createDataPartition(p=0.8,list=FALSE)
train <- df[train_samples,]
test <- df[-train_samples,]
model <- lm(V1~V2+V3+V4+V5+V6+V7+V8+V9+class,data=train)
summary(model)
print("RSE")
print(sigma(model))
#Make predictions
pred <- model %>%
predict(test)
#model performance
RMSE <- RMSE(test$V1,pred)
RMSE
R2 <- R2(pred,test$V1)
R2
### From the summary the significant predictors are class, V3 and V4
model <- lm(V1 ~ .,data = df)
summary(model)
print("RSE")
print(sigma(model))
### From the summary the significant predictors are class, V3 and V4
model <- lm(V1 ~ V2+V3+V4+V5+V6+V7+V8+V9+class,data = df)
summary(model)
print("RSE")
print(sigma(model))
### From the summary the significant predictors are class, V3 and V4
model <- lm(V1 ~ V2+V3+V4+V5+V6+V7+V8+V9+class,data = df)
summary(model)
print("RSE")
print(sigma(model))
### From the summary the significant predictors are class, V3 and V4
model <- lm(V1 ~ V3+V4+class,data = df)
print("RSE")
print(sigma(model))
rm(list=ls())
setwd("C:/FallSem 21-22/CSE3505 - Data Analytics/Project Prep/Sentiment Analysis of IMDb reviews")
data <- read.csv("movie_reviews.csv")
#head(data)
#View(data)
pacman::p_load(tm)
pacman::p_load(tmap)
pacman::p_load(SnowballC)
# str(data)
# corpus <- iconv(df$review, to="utf-8")
# Converting the reviews into a corpus in which each review can be treated as a document.
corpus <- Corpus(VectorSource(data$review))
#Lower case
corpus <- tm_map(corpus,tolower)
#Removing Numbers
corpus <- tm_map(corpus, removeNumbers)
#Removing punctuation
corpus <- tm_map(corpus, removePunctuation)
#Stripping white space
cleanset <- tm_map(corpus, stripWhitespace)
#Removing stop words
cleanset <- tm_map(cleanset, removeWords, stopwords("english"))
#Removing html tags
removeTags <- function(x) gsub('<.*?>', '', x)
cleanset <- tm_map(cleanset, content_transformer(removeTags))
wordsToRemove <- c("movie", "film", "one","make", "scene", "show","get","see","stories","watch","even","realli", "will", "character", "look","just","time", "made", "people", "first","thing","br","awaybr")
cleanset <- tm_map(cleanset, removeWords,wordsToRemove)
# Stemming
cleanset <- tm_map(cleanset,stemDocument)
# Creates a document matrix which is a Bag-of-Words model with Term Frequency
frequencies <- DocumentTermMatrix(cleanset)
#Remove sparse terms
#Only contains terms which are present in >=5% of the reviews
sparse <- removeSparseTerms(frequencies, 0.95)
#Converting the matrix into a dataframe
final_reviews <- as.data.frame(as.matrix((sparse)))
colnames(final_reviews) <- make.names(colnames(final_reviews))
cols <- names(final_reviews)
# Removing words which have a frequency less than 2000 in all the reviews
for (col in cols) {
if(sum(final_reviews[col]) < 2000) {
final_reviews[col] <- NULL
}
}
final_reviews$sentiment <- data$sentiment
pacman::p_load(e1071)
pacman::p_load(caret)
# Label encoding the categorical data
final_reviews$sentiment <- ifelse(final_reviews$sentiment == "negative",0,1)
#Analysis
frequency <- colSums(as.matrix(final_reviews[,-dim(final_reviews)[2]]))
freq <- sort(frequency, decreasing = TRUE)
words <- names(freq)
pacman::p_load(wordcloud)
wordcloud(words[1:100], freq[1:100],scale=c(1.5,.5), colors = c("red","green","blue"))
pacman::p_load(ggplot2)
barplot(freq[1:10],
main = "Most frequently used 10 words",
xlab = "Words",
ylab = "Frequency",
names.arg = words[1:10],
col = "darkred"
)
pacman::p_load(caTools)
set.seed(123)
# Splitting the data into training and testing
split = sample.split(final_reviews$sentiment, SplitRatio = 0.8)
training_set = subset(final_reviews, split == TRUE)
testing_data = subset(final_reviews, split == FALSE)
dims <- dim(training_set)
features_train <- training_set[,-dims[2]]
features_test <- testing_data[,-dims[2]]
labels_train <- training_set[,dims[2]]
labels_test <- testing_data[,dims[2]]
features_train_matrix <- data.matrix(features_train)
features_test_matrix <- data.matrix(features_test)
labels_train_matrix <- data.matrix(labels_train)
pacman::p_load(Metrics)
## Neural network using built in package
pacman::p_load(neuralnet)
n <- neuralnet(
sentiment ~ .,
data = training_set,
hidden = 1,
err.fct = "ce",
linear.output = FALSE
)
output <- compute(n, features_test)
p1 <- output$net.result
pred <- ifelse(p1>0.5, 1, 0)
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(testing_data$sentiment), positive="1")
recall <- sensitivity(as.factor(pred), as.factor(testing_data$sentiment), positive="1")
Accuracy
precision
recall
pacman::p_load(party)
# Create the tree.
/output.tree <- ctree(
sentiment ~ .,
data = training_set
)
# Create the tree.
output.tree <- ctree(
sentiment ~ .,
data = training_set
)
pred <- predict(output.tree, testing_data, type = 'response')
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(testing_data$sentiment), positive="1")
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive="1")
pred <- predict(output.tree, testing_data, type = 'response')
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive="1")
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive=1)
# Create the tree.
output.tree <- ctree(
sentiment ~ .,
data = training_set
)
unique(labels_test)
pred <- predict(output.tree, testing_data, type = 'response')
unique(pred)
pred <- predict(output.tree, testing_data, type = 'class')
pacman::p_load(tree)
# Create the tree.
output.tree <- tree(
sentiment ~ .,
data = training_set
)
pred <- predict(output.tree, testing_data, type = 'class')
# Create the tree.
output.tree <- ctree(
sentiment ~ .,
data = training_set
)
pred <- predict(output.tree, testing_data, type = 'class')
# Create the tree.
output.tree <- tree(
sentiment ~ .,
data = training_set
)
pred <- predict(output.tree, testing_data)
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive="1")
# Create the tree.
output.tree <- tree(
sentiment ~ .,
data = training_set
)
pred <- predict(output.tree, testing_data, type="class")
pacman::p_load(rpart)
tree <- rpart(sentiment ~., data = training_set)
pred <- predict(tree, testing_data, type="class")
pred <- predict(tree, testing_data, type="class")
training_set$sentiment <- factor(training_set$sentiment)
tree <- rpart(sentiment ~., data = training_set)
testing_data$sentiment <- factor(testing_data$sentiment)
pred <- predict(tree, testing_data, type="class")
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive="1")
recall <- sensitivity(as.factor(pred), as.factor(labels_test), positive="1")
Accuracy
precision
recall
rm(list=ls())
setwd("C:/FallSem 21-22/CSE3505 - Data Analytics/Project Prep/Sentiment Analysis of IMDb reviews")
data <- read.csv("movie_reviews.csv")
#head(data)
#View(data)
pacman::p_load(tm)
pacman::p_load(tmap)
pacman::p_load(SnowballC)
# str(data)
# corpus <- iconv(df$review, to="utf-8")
# Converting the reviews into a corpus in which each review can be treated as a document.
corpus <- Corpus(VectorSource(data$review))
#Lower case
corpus <- tm_map(corpus,tolower)
#Removing Numbers
corpus <- tm_map(corpus, removeNumbers)
#Removing punctuation
corpus <- tm_map(corpus, removePunctuation)
#Stripping white space
cleanset <- tm_map(corpus, stripWhitespace)
#Removing stop words
cleanset <- tm_map(cleanset, removeWords, stopwords("english"))
wordsToRemove <- c("movie", "film", "one","make", "scene", "show","get","see","stories","watch","even","realli", "will", "character", "look","just","time", "made", "people", "first","thing","br","awaybr")
cleanset <- tm_map(cleanset, removeWords,wordsToRemove)
# Stemming
cleanset <- tm_map(cleanset,stemDocument)
# Creates a document matrix which is a Bag-of-Words model with Term Frequency
frequencies <- DocumentTermMatrix(cleanset)
#Remove sparse terms
#Only contains terms which are present in >=5% of the reviews
sparse <- removeSparseTerms(frequencies, 0.95)
#Converting the matrix into a dataframe
final_reviews <- as.data.frame(as.matrix((sparse)))
colnames(final_reviews) <- make.names(colnames(final_reviews))
cols <- names(final_reviews)
# Removing words which have a frequency less than 2000 in all the reviews
for (col in cols) {
if(sum(final_reviews[col]) < 2000) {
final_reviews[col] <- NULL
}
}
final_reviews$sentiment <- data$sentiment
pacman::p_load(e1071)
pacman::p_load(caret)
# Label encoding the categorical data
final_reviews$sentiment <- ifelse(final_reviews$sentiment == "negative",0,1)
pacman::p_load(caTools)
set.seed(123)
# Splitting the data into training and testing
split = sample.split(final_reviews$sentiment, SplitRatio = 0.8)
training_set = subset(final_reviews, split == TRUE)
testing_data = subset(final_reviews, split == FALSE)
dims <- dim(training_set)
features_train <- training_set[,-dims[2]]
features_test <- testing_data[,-dims[2]]
labels_train <- training_set[,dims[2]]
labels_test <- testing_data[,dims[2]]
features_train_matrix <- data.matrix(features_train)
features_test_matrix <- data.matrix(features_test)
labels_train_matrix <- data.matrix(labels_train)
pacman::p_load(Metrics)
### Naive Bayes Classification ###
clf <- naiveBayes(sentiment ~ .,  data = training_set, laplace = 1)
#Predictions
y_pred <- predict(clf, features_test)
#Evaluation Metrics
precision <- posPredValue(as.factor(y_pred), as.factor(testing_data$sentiment), positive="1")
recall <- sensitivity(as.factor(y_pred), as.factor(testing_data$sentiment), positive="1")
Accuracy <- accuracy(testing_data$sentiment,y_pred)
Accuracy
precision
recall
pacman::p_load(glmnet)
pacman::p_load(dplyr)
#Logistic Regression with Lasso Regularization
cv.lasso <- cv.glmnet(features_train_matrix, labels_train_matrix, alpha = 1, family = "binomial")
model <- glmnet(features_train_matrix, labels_train_matrix, alpha = 1, family = "binomial",
lambda = cv.lasso$lambda.min)
probabilities <- model %>% predict(newx = features_test_matrix)
labels_pred <- ifelse(probabilities > 0.5, 1, 0)
#Evaluation Metrics
Accuracy <- accuracy(labels_test,labels_pred)
precision <- posPredValue(as.factor(labels_pred), as.factor(labels_test), positive="1")
recall <- sensitivity(as.factor(labels_pred), as.factor(labels_test), positive="1")
Accuracy
precision
recall
cv.ridge <- cv.glmnet(features_train_matrix, labels_train_matrix, alpha = 0, family = "binomial")
model <- glmnet(features_train_matrix, labels_train_matrix, alpha = 0, family = "binomial",
lambda = cv.ridge$lambda.min)
probabilities <- model %>% predict(newx = features_test_matrix)
labels_pred <- ifelse(probabilities > 0.5, 1, 0)
#Evaluation Metrics
Accuracy <- accuracy(labels_test,labels_pred)
precision <- posPredValue(as.factor(labels_pred), as.factor(labels_test), positive="1")
recall <- sensitivity(as.factor(labels_pred), as.factor(labels_test), positive="1")
Accuracy
precision
recall
## Logistic model without regularization
logistic_model <- glm(sentiment ~ .,
data = training_set,
family = "binomial")
predict_reg <- predict(logistic_model,
testing_data, type = "response")
predict_reg <- ifelse(predict_reg >0.5, 1, 0)
#Evaluation Metrics
Accuracy <- accuracy(labels_test,predict_reg)
precision <- posPredValue(as.factor(predict_reg), as.factor(labels_test), positive="1")
recall <- sensitivity(as.factor(predict_reg), as.factor(labels_test), positive="1")
Accuracy
precision
recall
## Neural network using built in package
pacman::p_load(neuralnet)
pacman::p_load(rpart)
# Decision Tree
training_set$sentiment <- factor(training_set$sentiment)
tree <- rpart(sentiment ~., data = training_set)
testing_data$sentiment <- factor(testing_data$sentiment)
pred <- predict(tree, testing_data, type="class")
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(labels_test), positive="1")
recall <- sensitivity(as.factor(pred), as.factor(labels_test), positive="1")
Accuracy
precision
recall
n <- neuralnet(
sentiment ~ .,
data = training_set,
hidden = 1,
err.fct = "ce",
linear.output = FALSE
)
output <- compute(n, features_test)
output <- compute(n, features_test)
final_reviews$sentiment <- data$sentiment
# Label encoding the categorical data
final_reviews$sentiment <- ifelse(final_reviews$sentiment == "negative",0,1)
# Splitting the data into training and testing
split = sample.split(final_reviews$sentiment, SplitRatio = 0.8)
training_set = subset(final_reviews, split == TRUE)
testing_data = subset(final_reviews, split == FALSE)
dims <- dim(training_set)
features_train <- training_set[,-dims[2]]
features_test <- testing_data[,-dims[2]]
labels_train <- training_set[,dims[2]]
labels_test <- testing_data[,dims[2]]
features_train_matrix <- data.matrix(features_train)
features_test_matrix <- data.matrix(features_test)
labels_train_matrix <- data.matrix(labels_train)
n <- neuralnet(
sentiment ~ .,
data = training_set,
hidden = 1,
err.fct = "ce",
linear.output = FALSE
)
output <- compute(n, features_test)
output <- neuralnet::compute(n, features_test)
p1 <- output$net.result
pred <- ifelse(p1>0.5, 1, 0)
Accuracy <- accuracy(labels_test, pred)
precision <- posPredValue(as.factor(pred), as.factor(testing_data$sentiment), positive="1")
recall <- sensitivity(as.factor(pred), as.factor(testing_data$sentiment), positive="1")
Accuracy
precision
recall
